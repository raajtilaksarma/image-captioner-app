{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Captitioning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timothykmulenga/image-captioner-app/blob/master/Image_Captitioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGxsINrZCuM5",
        "colab_type": "text"
      },
      "source": [
        "# Image Captioning in **Pytorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kKVqyn9D7NQ",
        "colab_type": "text"
      },
      "source": [
        "This notebook uses a pre-trained **resnet-152 model** is used as an **encoder**, and the **decoder** is an **LSTM network.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6bCmI7NEpUd",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/03-advanced/image_captioning/png/model.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3tX_MZGFrgz",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Setting up the system\n",
        "\n",
        "\n",
        "#NB: RUN EACH LINE INDIVUDUALY NOT AS A SINGLE CELL!!\n",
        "!git clone https://github.com/pdollar/coco.git\n",
        "\n",
        "cd coco/PythonAPI/\n",
        "\n",
        "python setup.py build\n",
        "python setup.py install\n",
        "\n",
        "\n",
        "cd ../../\n",
        "\n",
        "!git clone https://github.com/yunjey/pytorch-tutorial.git\n",
        "cd pytorch-tutorial/tutorials/03-advanced/image_captioning/\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaHjwr2WBwNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a50a89f8-3b5b-47ac-98c7-5a690dd54472"
      },
      "source": [
        "!git clone https://github.com/pdollar/coco.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'coco'...\n",
            "remote: Enumerating objects: 975, done.\u001b[K\n",
            "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975\n",
            "Receiving objects: 100% (975/975), 11.72 MiB | 7.65 MiB/s, done.\n",
            "Resolving deltas: 100% (576/576), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlfg0MewBxVH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3dd7fdb-ebb7-4bd1-9a03-a936f3089c23"
      },
      "source": [
        "cd coco/PythonAPI/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/coco/PythonAPI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w93FUJHBxEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py build"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEXJJhzxBwx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dctajrCmCHl4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40901eb4-8444-4358-fc6f-ba3acadfeb6f"
      },
      "source": [
        "cd ../../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZXThrK0CHMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/yunjey/pytorch-tutorial.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULr_YybsCQhJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "142ee74c-0cba-4656-f5bb-0e2cd91fc6f9"
      },
      "source": [
        "cd pytorch-tutorial/tutorials/03-advanced/image_captioning/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pytorch-tutorial/tutorials/03-advanced/image_captioning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2_mp2prCbFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqhg_Jt6uoD1",
        "colab_type": "text"
      },
      "source": [
        "## Photo and Caption Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJyQ49lzJ7d9",
        "colab_type": "text"
      },
      "source": [
        "MS-COCO dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk-Erz5aKYN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/yunjey/pytorch-tutorial.git\n",
        "%cd pytorch-tutorial/tutorials/03-advanced/image_captioning/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Bf-riquvQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dowloading the dataset\n",
        "!chmod +x download.sh\n",
        "!./download.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-uJXrBjdv0t",
        "colab_type": "text"
      },
      "source": [
        "# Process The Input\n",
        "\n",
        "Our input  include images and textural data for captioning\n",
        "\n",
        "## Prepare Photo Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jy6bvi6vIzl",
        "colab_type": "text"
      },
      "source": [
        "Common practice to use a pre-trained model to interpret the content of the photos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD1FPgdBwii6",
        "colab_type": "text"
      },
      "source": [
        " Using the pre-trained model, we can pre-compute the “photo features” and save them to file. We can then load these features later and feed them into our model as the interpretation of a given photo in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MWYB79SvCpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resize all the images to bring them to shape 224x224\n",
        "#Do not run these cell\n",
        "python resize.py \n",
        "\n",
        "'''below are contents of this file'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dga9SoEbYaj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def resize_image(image, size):\n",
        "    \"\"\"Resize an image to the given size.\"\"\"\n",
        "    return image.resize(size, Image.ANTIALIAS)\n",
        "\n",
        "def resize_images(image_dir, output_dir, size):\n",
        "    \"\"\"Resize the images in 'image_dir' and save into 'output_dir'.\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    images = os.listdir(image_dir) #read in the images\n",
        "    num_images = len(images)\n",
        "    for i, image in enumerate(images):\n",
        "        with open(os.path.join(image_dir, image), 'r+b') as f:\n",
        "            with Image.open(f) as img:\n",
        "                img = resize_image(img, size)\n",
        "                img.save(os.path.join(output_dir, image), img.format)\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (\"[{}/{}] Resized the images and saved into '{}'.\"\n",
        "                   .format(i+1, num_images, output_dir))\n",
        "\n",
        "def main(args):\n",
        "    image_dir = args.image_dir\n",
        "    output_dir = args.output_dir\n",
        "    image_size = [args.image_size, args.image_size]\n",
        "    resize_images(image_dir, output_dir, image_size)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--image_dir', type=str, default='./data/train2014/',\n",
        "                        help='directory for train images')\n",
        "    parser.add_argument('--output_dir', type=str, default='./data/resized2014/',\n",
        "                        help='directory for saving resized images')\n",
        "    parser.add_argument('--image_size', type=int, default=256,\n",
        "                        help='size for image after processing')\n",
        "    args = parser.parse_args()\n",
        "    main(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbu7nTMhzezG",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlNVqB4czfvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Search for all the possible words in the dataset and \n",
        "# build a vocabulary list\n",
        "#do not run these cell\n",
        "python build_vocab.py  \n",
        "\n",
        "'''below are contents of these file'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9YnXXVXYDxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import pickle\n",
        "import argparse\n",
        "from collections import Counter\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "def build_vocab(json, threshold):\n",
        "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
        "    coco = COCO(json)\n",
        "    counter = Counter()\n",
        "    ids = coco.anns.keys()\n",
        "    for i, id in enumerate(ids):\n",
        "        caption = str(coco.anns[id]['caption'])\n",
        "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
        "        counter.update(tokens)\n",
        "\n",
        "        if (i+1) % 1000 == 0:\n",
        "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
        "\n",
        "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
        "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
        "\n",
        "    # Create a vocab wrapper and add some special tokens.\n",
        "    vocab = Vocabulary()\n",
        "    vocab.add_word('<pad>')\n",
        "    vocab.add_word('<start>')\n",
        "    vocab.add_word('<end>')\n",
        "    vocab.add_word('<unk>')\n",
        "\n",
        "    # Add the words to the vocabulary.\n",
        "    for i, word in enumerate(words):\n",
        "        vocab.add_word(word)\n",
        "    return vocab\n",
        "\n",
        "def main(args):\n",
        "    vocab = build_vocab(json=args.caption_path, threshold=args.threshold)\n",
        "    vocab_path = args.vocab_path\n",
        "    with open(vocab_path, 'wb') as f:\n",
        "        pickle.dump(vocab, f)\n",
        "    print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
        "    print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--caption_path', type=str, \n",
        "                        default='data/annotations/captions_train2014.json', \n",
        "                        help='path for train annotation file')\n",
        "    parser.add_argument('--vocab_path', type=str, default='./data/vocab.pkl', \n",
        "                        help='path for saving vocabulary wrapper')\n",
        "    parser.add_argument('--threshold', type=int, default=4, \n",
        "                        help='minimum word count threshold')\n",
        "    args = parser.parse_args()\n",
        "    main(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxfJNfUQz9Uu",
        "colab_type": "text"
      },
      "source": [
        "## Develop Deep Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLAbhzUjM_Z3",
        "colab_type": "text"
      },
      "source": [
        "For the encoder part, the pretrained CNN extracts the feature vector from a given input image. The feature vector is linearly transformed to have the same dimension as the input dimension of the LSTM network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAhCEViyNJ1s",
        "colab_type": "text"
      },
      "source": [
        "For the decoder part, source and target texts are predefined. For example, if the image description is \"Giraffes standing next to each other\", the source sequence is a list containing ['<start>', 'Giraffes', 'standing', 'next', 'to', 'each', 'other'] and the target sequence is a list containing ['Giraffes', 'standing', 'next', 'to', 'each', 'other', '<end>']. Using these source and target sequences and the feature vector, the LSTM decoder is trained as a language model conditioned on the feature vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UYITqgT0IFi",
        "colab_type": "text"
      },
      "source": [
        "First, we must load the prepared photo and text data so that we can use it to fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b63LrbmoY7rC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "        \n",
        "    def forward(self, images):\n",
        "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "        features = features.reshape(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "        return features\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
        "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.max_seg_length = max_seq_length\n",
        "        \n",
        "    def forward(self, features, captions, lengths):\n",
        "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
        "        hiddens, _ = self.lstm(packed)\n",
        "        outputs = self.linear(hiddens[0])\n",
        "        return outputs\n",
        "    \n",
        "    def sample(self, features, states=None):\n",
        "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
        "        sampled_ids = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "        for i in range(self.max_seg_length):\n",
        "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
        "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
        "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
        "            sampled_ids.append(predicted)\n",
        "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
        "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
        "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
        "        return sampled_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edmM19kNhV-7",
        "colab_type": "text"
      },
      "source": [
        "## Train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD9408Scz-SU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from data_loader import get_loader \n",
        "from build_vocab import Vocabulary\n",
        "from model import EncoderCNN, DecoderRNN\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def main(args):\n",
        "    # Create model directory\n",
        "    if not os.path.exists(args.model_path):\n",
        "        os.makedirs(args.model_path)\n",
        "    \n",
        "    # Image preprocessing, normalization for the pretrained resnet\n",
        "    transform = transforms.Compose([ \n",
        "        transforms.RandomCrop(args.crop_size),\n",
        "        transforms.RandomHorizontalFlip(), \n",
        "        transforms.ToTensor(), \n",
        "        transforms.Normalize((0.485, 0.456, 0.406), \n",
        "                             (0.229, 0.224, 0.225))])\n",
        "    \n",
        "    # Load vocabulary wrapper\n",
        "    with open(args.vocab_path, 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "    \n",
        "    # Build data loader\n",
        "    data_loader = get_loader(args.image_dir, args.caption_path, vocab, \n",
        "                             transform, args.batch_size,\n",
        "                             shuffle=True, num_workers=args.num_workers) \n",
        "\n",
        "    # Build the models\n",
        "    encoder = EncoderCNN(args.embed_size).to(device)\n",
        "    decoder = DecoderRNN(args.embed_size, args.hidden_size, len(vocab), args.num_layers).to(device)\n",
        "    \n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
        "    optimizer = torch.optim.Adam(params, lr=args.learning_rate)\n",
        "    \n",
        "    # Train the models\n",
        "    total_step = len(data_loader)\n",
        "    for epoch in range(args.num_epochs):\n",
        "        for i, (images, captions, lengths) in enumerate(data_loader):\n",
        "            \n",
        "            # Set mini-batch dataset\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
        "            \n",
        "            # Forward, backward and optimize\n",
        "            features = encoder(images)\n",
        "            outputs = decoder(features, captions, lengths)\n",
        "            loss = criterion(outputs, targets)\n",
        "            decoder.zero_grad()\n",
        "            encoder.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print log info\n",
        "            if i % args.log_step == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
        "                      .format(epoch, args.num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n",
        "                \n",
        "            # Save the model checkpoints\n",
        "            if (i+1) % args.save_step == 0:\n",
        "                torch.save(decoder.state_dict(), os.path.join(\n",
        "                    args.model_path, 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "                torch.save(encoder.state_dict(), os.path.join(\n",
        "                    args.model_path, 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--model_path', type=str, default='models/' , help='path for saving trained models')\n",
        "    parser.add_argument('--crop_size', type=int, default=224 , help='size for randomly cropping images')\n",
        "    parser.add_argument('--vocab_path', type=str, default='data/vocab.pkl', help='path for vocabulary wrapper')\n",
        "    parser.add_argument('--image_dir', type=str, default='data/resized2014', help='directory for resized images')\n",
        "    parser.add_argument('--caption_path', type=str, default='data/annotations/captions_train2014.json', help='path for train annotation json file')\n",
        "    parser.add_argument('--log_step', type=int , default=10, help='step size for prining log info')\n",
        "    parser.add_argument('--save_step', type=int , default=1000, help='step size for saving trained models')\n",
        "    \n",
        "    # Model parameters\n",
        "    parser.add_argument('--embed_size', type=int , default=256, help='dimension of word embedding vectors')\n",
        "    parser.add_argument('--hidden_size', type=int , default=512, help='dimension of lstm hidden states')\n",
        "    parser.add_argument('--num_layers', type=int , default=1, help='number of layers in lstm')\n",
        "    \n",
        "    parser.add_argument('--num_epochs', type=int, default=5)\n",
        "    parser.add_argument('--batch_size', type=int, default=128)\n",
        "    parser.add_argument('--num_workers', type=int, default=2)\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.001)\n",
        "    args = parser.parse_args()\n",
        "    print(args)\n",
        "    main(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XBJrx7vCsAe",
        "colab_type": "text"
      },
      "source": [
        "#Testing our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GKK69qlh2tI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "8069a63b-03d6-46bb-a083-90d564e4f544"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import argparse\n",
        "import pickle \n",
        "import os\n",
        "from torchvision import transforms \n",
        "from build_vocab import Vocabulary\n",
        "from model import EncoderCNN, DecoderRNN\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def load_image(image_path, transform=None):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = image.resize([224, 224], Image.LANCZOS)\n",
        "    \n",
        "    if transform is not None:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "    \n",
        "    return image\n",
        "\n",
        "def main(args):\n",
        "    # Image preprocessing\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(), \n",
        "        transforms.Normalize((0.485, 0.456, 0.406), \n",
        "                             (0.229, 0.224, 0.225))])\n",
        "    \n",
        "    # Load vocabulary wrapper\n",
        "    with open(args.vocab_path, 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "\n",
        "    # Build models\n",
        "    encoder = EncoderCNN(args.embed_size).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
        "    decoder = DecoderRNN(args.embed_size, args.hidden_size, len(vocab), args.num_layers)\n",
        "    encoder = encoder.to(device)\n",
        "    decoder = decoder.to(device)\n",
        "\n",
        "    # Load the trained model parameters\n",
        "    encoder.load_state_dict(torch.load(args.encoder_path))\n",
        "    decoder.load_state_dict(torch.load(args.decoder_path))\n",
        "\n",
        "    # Prepare an image\n",
        "    image = load_image(args.image, transform)\n",
        "    image_tensor = image.to(device)\n",
        "    \n",
        "    # Generate an caption from the image\n",
        "    feature = encoder(image_tensor)\n",
        "    sampled_ids = decoder.sample(feature)\n",
        "    sampled_ids = sampled_ids[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n",
        "    \n",
        "    # Convert word_ids to words\n",
        "    sampled_caption = []\n",
        "    for word_id in sampled_ids:\n",
        "        word = vocab.idx2word[word_id]\n",
        "        sampled_caption.append(word)\n",
        "        if word == '<end>':\n",
        "            break\n",
        "    sentence = ' '.join(sampled_caption)\n",
        "    \n",
        "    # Print out the image and the generated caption\n",
        "    print (sentence)\n",
        "    image = Image.open(args.image)\n",
        "    plt.imshow(np.asarray(image))\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--image', type=str, required=True, help='input image for generating caption')\n",
        "    parser.add_argument('--encoder_path', type=str, default='/content/models/encoder-5-3000.pkl', help='path for trained encoder')\n",
        "    parser.add_argument('--decoder_path', type=str, default='/content/models/decoder-5-3000.pkl', help='path for trained decoder')\n",
        "    parser.add_argument('--vocab_path', type=str, default='/content/data/vocab.pkl', help='path for vocabulary wrapper')\n",
        "    \n",
        "    # Model parameters (should be same as paramters in train.py)\n",
        "    parser.add_argument('--embed_size', type=int , default=256, help='dimension of word embedding vectors')\n",
        "    parser.add_argument('--hidden_size', type=int , default=512, help='dimension of lstm hidden states')\n",
        "    parser.add_argument('--num_layers', type=int , default=1, help='number of layers in lstm')\n",
        "    args = parser.parse_args()\n",
        "    main(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-12bdd1818d4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbuild_vocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoderCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'build_vocab'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}